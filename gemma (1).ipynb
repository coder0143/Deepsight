{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d536aae1a694103bf3ccc750ae4bf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = '../dataset/'\n",
    "train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.84637333333333\n",
      "97.0\n",
      "1322\n",
      "First Quartile (Q1): 37.0\n",
      "Third Quartile (Q3): 201.0\n",
      "Interquartile Range (IQR): 164.0\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(i.split(' ')) for i in train[\"catalog_content\"]]\n",
    "Q1 = np.percentile(lengths, 25)\n",
    "Q3 = np.percentile(lengths, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(np.mean(lengths))\n",
    "print(np.median(lengths))\n",
    "print(max(lengths))\n",
    "print(f\"First Quartile (Q1): {Q1}\")\n",
    "print(f\"Third Quartile (Q3): {Q3}\")\n",
    "print(f\"Interquartile Range (IQR): {IQR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    75000.000000\n",
       "mean        23.647654\n",
       "std         33.376932\n",
       "min          0.130000\n",
       "25%          6.795000\n",
       "50%         14.000000\n",
       "75%         28.625000\n",
       "max       2796.000000\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"price\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.amp.autocast_mode import autocast \n",
    "from torch.amp.grad_scaler import GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# from transformers import AutoProcessor, AutoModel\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred, eps=1e-8):\n",
    "    # y_true, y_pred: numpy arrays or tensors on CPU\n",
    "    y_true = np.array(y_true).astype(np.float64)\n",
    "    y_pred = np.array(y_pred).astype(np.float64)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps)\n",
    "    return 100.0 * np.mean(2.0 * np.abs(y_true - y_pred) / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductPriceDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, image_root: str):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_root = image_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_name = row[\"image_link\"].split(\"/\")[-1]\n",
    "        img_path = os.path.join(self.image_root, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            # Fallback image (solid black) if corrupted or missing\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        text = str(row['catalog_content'])\n",
    "        if len(text.strip()) == 0:\n",
    "            text = \"No description available.\"\n",
    "\n",
    "        # processor could handle both image and text tokenization\n",
    "        # but here we return raw and do processor in collate_fn\n",
    "        return {'image':image, 'text':text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycollate_fn(batch):\n",
    "    imgs = [b['image'] for b in batch]\n",
    "    texts = [b['text'] for b in batch]\n",
    "    return {'images':imgs, 'texts': texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ProductPriceDataset(train, image_root='train_images')\n",
    "train_loader = DataLoader(dataset_train, batch_size=100, collate_fn=lambda b: mycollate_fn(b), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ProductPriceDataset(test, image_root='test_images') \n",
    "test_loader = DataLoader(dataset_test, batch_size=100, collate_fn=lambda b: mycollate_fn(b), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "siglip_model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16)\n",
    "siglip_model.to(device)\n",
    "emb_text = SentenceTransformer(\"google/embeddinggemma-300m\").to(device)\n",
    "processor_i = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00562ad8969488091f8ba6d386fceaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Saved!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12f9b4d3b41476aae90d42c65616975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Saved!\n"
     ]
    }
   ],
   "source": [
    "siglip_model.eval()\n",
    "emb_text.eval()\n",
    "\n",
    "img_embds_train = []\n",
    "text_embds_train = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(train_loader):\n",
    "        img = processor_i(images=batch['images'], return_tensors=\"pt\")\n",
    "        img = img.to(device)\n",
    "        img_embd = siglip_model.get_image_features(**img)\n",
    "        img_embds_train.extend(img_embd.to(torch.float32).cpu().squeeze().numpy())\n",
    "        text_embd = emb_text.encode_document(batch['texts'])\n",
    "        text_embds_train.extend(text_embd)\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"Train_embeddings_image_text.npz\",\n",
    "    img_embds_train=np.array(img_embds_train, dtype=object),\n",
    "    text_embds_train=np.array(text_embds_train, dtype=object)\n",
    ")\n",
    "\n",
    "del img_embds_train, text_embds_train\n",
    "print(\"Train Saved!\")\n",
    "\n",
    "img_embds_test = []\n",
    "text_embds_test = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_loader):\n",
    "        img = processor_i(images=batch['images'], return_tensors=\"pt\")\n",
    "        img = img.to(device)\n",
    "        img_embd = siglip_model.get_image_features(**img)\n",
    "        img_embds_test.extend(img_embd.to(torch.float32).cpu().squeeze().numpy())\n",
    "        text_embd = emb_text.encode_document(batch['texts'])\n",
    "        text_embds_test.extend(text_embd)\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"Test_embeddings_image_text.npz\",\n",
    "    img_embds_test=np.array(img_embds_test, dtype=object),\n",
    "    text_embds_test=np.array(text_embds_test, dtype=object)\n",
    ")\n",
    "\n",
    "del img_embds_test, text_embds_test\n",
    "print(\"Test Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the extracted embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 768) (75000, 768)\n"
     ]
    }
   ],
   "source": [
    "# Loading from numpy npz\n",
    "loaded_data = np.load(\"Train_embeddings_image_text.npz\", allow_pickle=True)\n",
    "img_train = loaded_data['img_embds_train']\n",
    "text_train = loaded_data['text_embds_train']\n",
    "print(img_train.shape, text_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBED_DIM = 768\n",
    "# LATENT_DIM = 512    # Dimension for projections and attention\n",
    "# NUM_HEADS = 8       # MultiHeadAttention heads\n",
    "# MLP_DIM = 1024      # Inner dimension for MLP blocks\n",
    "# DROPOUT_RATE = 0.1\n",
    "# BATCH_SIZE = 150\n",
    "# LEARNING_RATE = 4e-5\n",
    "# EPOCHS = 50         # Training for a fixed number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 768\n",
    "LATENT_DIM = 512    # Dimension for projections and attention\n",
    "NUM_HEADS = 8       # MultiHeadAttention heads\n",
    "MLP_DIM = 2048      # Inner dimension for MLP blocks\n",
    "DROPOUT_RATE = 0.1\n",
    "BATCH_SIZE = 150\n",
    "LEARNING_RATE = 4e-5\n",
    "EPOCHS = 100         # Training for a fixed number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_metric(y_pred_raw, y_true_raw):\n",
    "    \"\"\"\n",
    "    SMAPE (Symmetric Mean Absolute Percentage Error) - for evaluation.\n",
    "    Requires raw (non-log) prices.\n",
    "    \"\"\"\n",
    "    # Ensure y_true is a tensor on the same device as y_pred\n",
    "    if isinstance(y_true_raw, np.ndarray):\n",
    "        y_true = torch.tensor(y_true_raw, dtype=torch.float32, device=y_pred_raw.device)\n",
    "    else:\n",
    "        y_true = y_true_raw\n",
    "        \n",
    "    y_pred = y_pred_raw\n",
    "    \n",
    "    # Clamp predictions to a minimum value (e.g., 0.13) before SMAPE calculation\n",
    "    MIN_PRICE_CLAMP = 0.13\n",
    "    y_pred = torch.clamp(y_pred, min=MIN_PRICE_CLAMP)\n",
    "\n",
    "    numerator = torch.abs(y_pred - y_true)\n",
    "    denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2.0\n",
    "    \n",
    "    # SMAPE calculation (100 * mean of percentage error)\n",
    "    smape = 100 * torch.mean(numerator / (denominator + 1e-7)) # Added 1e-7 for stability\n",
    "    return smape.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Transformer-like MLP with LayerNorm and Residual connection.\"\"\"\n",
    "    def __init__(self, in_features, mlp_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(in_features)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_dim, in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.net(self.norm(x)) # Residual connection and pre-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceFusionTransformer(nn.Module):\n",
    "    \"\"\"Cross-Attention Fusion Network for Multimodal Regression.\"\"\"\n",
    "    def __init__(self, embed_dim=768, latent_dim=512, num_heads=8, mlp_dim=2048, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Input Projections\n",
    "        self.img_proj = nn.Linear(embed_dim, latent_dim)\n",
    "        self.text_proj = nn.Linear(embed_dim, latent_dim)\n",
    "        \n",
    "        # 2. Cross-Attention Fusion Block\n",
    "        # We model text as Q attending to Image (K, V) and Image as Q attending to Text (K, V)\n",
    "        self.cross_attn_img_to_text = nn.MultiheadAttention(latent_dim, num_heads, dropout=0.05, batch_first=True)\n",
    "        self.cross_attn_text_to_img = nn.MultiheadAttention(latent_dim, num_heads, dropout=0.05, batch_first=True)\n",
    "        \n",
    "        # LayerNorm for the Attention outputs (Post-Attention Norm)\n",
    "        self.norm_img = nn.LayerNorm(latent_dim)\n",
    "        self.norm_text = nn.LayerNorm(latent_dim)\n",
    "\n",
    "        # 3. Final Fusion & Regression Head\n",
    "        self.fusion_norm = nn.LayerNorm(latent_dim * 2) # Latent dim x 2 after concatenation\n",
    "        \n",
    "        # MLP stack on the fused embedding\n",
    "        self.mlp_block1 = MLPBlock(latent_dim * 2, mlp_dim, dropout_rate)\n",
    "        self.mlp_block2 = MLPBlock(latent_dim * 2, mlp_dim, dropout_rate)\n",
    "        \n",
    "        # Output is a single log-transformed price value\n",
    "        self.regression_head = nn.Linear(latent_dim * 2, 1)\n",
    "\n",
    "    def forward(self, img_emb, text_emb):\n",
    "        \n",
    "        # 1. Projections\n",
    "        E_img = self.img_proj(img_emb).unsqueeze(1)    # Bx1xL\n",
    "        E_text = self.text_proj(text_emb).unsqueeze(1)  # Bx1xL\n",
    "\n",
    "        # 2. Cross-Attention\n",
    "        \n",
    "        # Text-to-Image (Text Query to Image Key/Value)\n",
    "        attn_output_t, _ = self.cross_attn_text_to_img(E_text, E_img, E_img)\n",
    "        fused_text = self.norm_text(E_text + attn_output_t) # Residual + Post-norm\n",
    "\n",
    "        # Image-to-Text (Image Query to Text Key/Value)\n",
    "        attn_output_i, _ = self.cross_attn_img_to_text(E_img, E_text, E_text)\n",
    "        fused_img = self.norm_img(E_img + attn_output_i) # Residual + Post-norm\n",
    "\n",
    "        # 3. Final Fusion (Concatenation)\n",
    "        E_final = torch.cat([fused_text.squeeze(1), fused_img.squeeze(1)], dim=-1) # Bx(2*L)\n",
    "        E_final = self.fusion_norm(E_final)\n",
    "        \n",
    "        # MLP Blocks\n",
    "        E_final = self.mlp_block1(E_final)\n",
    "        E_final = self.mlp_block2(E_final)\n",
    "        \n",
    "        # Regression Head: Output is log1p(price)\n",
    "        y_pred_log = self.regression_head(E_final)\n",
    "        \n",
    "        return y_pred_log.squeeze(1) # Bx1 -> B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(img_train, text_train, prices):\n",
    "    # --- 3.2 Target Transformation (log1p) ---\n",
    "    # Log transform the price target\n",
    "    # np.log1p(x) is equivalent to np.log(1 + x)\n",
    "    prices_log = np.log1p(prices)\n",
    "    \n",
    "    # Convert to PyTorch Tensors\n",
    "    X_img_tensor = torch.tensor(img_train.astype(np.float32), dtype=torch.float32)\n",
    "    X_text_tensor = torch.tensor(text_train.astype(np.float32), dtype=torch.float32)\n",
    "    Y_tensor = torch.tensor(prices_log, dtype=torch.float32)\n",
    "    \n",
    "    # Create PyTorch Dataset and DataLoader\n",
    "    full_dataset = TensorDataset(X_img_tensor, X_text_tensor, Y_tensor)\n",
    "    full_dataloader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Return the scaler objects for transforming the test data later\n",
    "    return full_dataloader, np.array(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_smape(model, data_loader, raw_prices):\n",
    "    model.eval()\n",
    "    y_pred_log_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_batch, text_batch, _ in data_loader: # Iterate over the DataLoader\n",
    "            img_batch = img_batch.to(DEVICE)\n",
    "            text_batch = text_batch.to(DEVICE)\n",
    "            y_pred_log = model(img_batch, text_batch)\n",
    "            y_pred_log_list.append(y_pred_log.cpu()) # Keep on CPU for concatenation/memory\n",
    "\n",
    "    y_pred_log_all = torch.cat(y_pred_log_list)\n",
    "    y_pred_raw_all = torch.expm1(y_pred_log_all).to(DEVICE) # Convert back to device for SMAPE calc\n",
    "    \n",
    "    # We don't have R2 here, but we return the SMAPE\n",
    "    smape_score = smape_metric(y_pred_raw_all, raw_prices)\n",
    "    \n",
    "    return smape_score\n",
    "\n",
    "def calculate_smape_on_train_set(model, train_loader, prices_train_raw):\n",
    "    \n",
    "    # 1. Calculate Training SMAPE\n",
    "    train_smape = get_predictions_and_smape(model, train_loader, prices_train_raw)\n",
    "\n",
    "    model.train() # Switch model back to train mode\n",
    "    return train_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, prices_raw_array, val_loader, price_val_raw, epochs=EPOCHS, lr=LEARNING_RATE):\n",
    "    \n",
    "    # Initialize Model, Optimizer, and Scheduler\n",
    "    model = PriceFusionTransformer(\n",
    "        embed_dim=EMBED_DIM, latent_dim=LATENT_DIM, num_heads=NUM_HEADS, mlp_dim=MLP_DIM, dropout_rate=DROPOUT_RATE\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # AdamW is the recommended robust optimizer for Transformer architectures\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-8) \n",
    "\n",
    "    # Loss\n",
    "    loss_fn = nn.SmoothL1Loss(beta=0.1)\n",
    "    \n",
    "    # Cosine Annealing Scheduler (with Warmup for stability)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2, eta_min=1e-8)\n",
    "    \n",
    "    print(f\"Starting training on full dataset for {epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for img_batch, text_batch, y_true_log in dataloader:\n",
    "            \n",
    "            img_batch = img_batch.to(DEVICE)\n",
    "            text_batch = text_batch.to(DEVICE)\n",
    "            y_true_log = y_true_log.to(DEVICE)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: model outputs log1p(price)\n",
    "            y_pred_log = model(img_batch, text_batch)\n",
    "            \n",
    "            # Calculate custom Hinge Regression Loss\n",
    "            loss = loss_fn(y_pred_log, y_true_log)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent explosion (common in custom loss/transformers)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * img_batch.size(0)\n",
    "            \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "\n",
    "        train_smape, val_smape = calculate_smape_on_train_set(model, dataloader, prices_raw_array, val_loader, price_val_raw)\n",
    "        print(f\"Epoch: {epoch+1:5d} | {avg_loss:.4f} | {train_smape:.4f}% | {val_smape:.4f} | {scheduler.get_last_lr()[0]:.8f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_no_val(dataloader, prices_train_raw, epochs=EPOCHS, lr=LEARNING_RATE):\n",
    "    \n",
    "    # Initialize Model, Optimizer, and Scheduler\n",
    "    model = PriceFusionTransformer(\n",
    "        embed_dim=EMBED_DIM, latent_dim=LATENT_DIM, num_heads=NUM_HEADS, mlp_dim=MLP_DIM, dropout_rate=DROPOUT_RATE\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # AdamW is the recommended robust optimizer for Transformer architectures\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-8) \n",
    "\n",
    "    # Loss\n",
    "    loss_fn = nn.SmoothL1Loss(beta=0.1)\n",
    "    \n",
    "    # Cosine Annealing Scheduler (with Warmup for stability)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2, eta_min=1e-8)\n",
    "    \n",
    "    print(f\"Starting training on full dataset for {epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for img_batch, text_batch, y_true_log in dataloader:\n",
    "            \n",
    "            img_batch = img_batch.to(DEVICE)\n",
    "            text_batch = text_batch.to(DEVICE)\n",
    "            y_true_log = y_true_log.to(DEVICE)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: model outputs log1p(price)\n",
    "            y_pred_log = model(img_batch, text_batch)\n",
    "            \n",
    "            # Calculate custom Hinge Regression Loss\n",
    "            loss = loss_fn(y_pred_log, y_true_log)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent explosion (common in custom loss/transformers)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * img_batch.size(0)\n",
    "            \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "\n",
    "        train_smape = get_predictions_and_smape(model, train_loader, prices_train_raw)\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1:5d} | {avg_loss:.4f} | {train_smape:.4f}% | {scheduler.get_last_lr()[0]:.8f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set(model, img_test, text_test):\n",
    "    \n",
    "    # Convert to Tensors\n",
    "    X_img_test = torch.tensor(img_test.astype(np.float32), dtype=torch.float32).to(DEVICE)\n",
    "    X_text_test = torch.tensor(text_test.astype(np.float32), dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    # Create DataLoader for batch inference\n",
    "    test_dataset = TensorDataset(X_img_test, X_text_test)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    predictions_log = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_batch, text_batch in test_dataloader:\n",
    "            \n",
    "            # Model outputs log1p(price)\n",
    "            y_pred_log = model(img_batch, text_batch)\n",
    "            \n",
    "            # Collect predictions\n",
    "            predictions_log.append(y_pred_log.cpu().numpy())\n",
    "            \n",
    "    # Concatenate all batch results\n",
    "    predictions_log = np.concatenate(predictions_log)\n",
    "    \n",
    "    # Reverse transformation: price = expm1(log1p(price))\n",
    "    # np.expm1(x) is equivalent to np.exp(x) - 1\n",
    "    final_predictions_raw = np.expm1(predictions_log)\n",
    "    \n",
    "    # Ensure final predictions are non-negative and meet the minimum price constraint\n",
    "    MIN_PRICE = 0.13\n",
    "    final_predictions_raw = np.maximum(final_predictions_raw, MIN_PRICE)\n",
    "    \n",
    "    return final_predictions_raw.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_data = train[\"price\"].tolist()\n",
    "# full_dataloader, full_dataset, prices_raw_array, val_set, prices_val_raw = prepare_data_for_kfold(img_train, text_train, prices_data, part_val_indices=, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_indices = np.arange(len(train))\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Split: {i}\")\n",
    "    _, part_val_indices = train_test_split(\n",
    "    all_indices, \n",
    "    test_size=0.1, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    "    )\n",
    "    full_dataloader, prices_raw_array, val_loader, prices_val_raw = prepare_data_for_kfold(img_train, text_train, prices_data, part_val_indices=part_val_indices, BATCH_SIZE=150)\n",
    "    model = train_model(full_dataloader, prices_raw_array, val_loader, prices_val_raw, epochs=EPOCHS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, prices_train_raw = prepare_data(img_train, text_train, prices_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on full dataset for 60 epochs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     1 | 0.6685 | 66.1939% | 0.00003975\n",
      "Epoch:     2 | 0.6185 | 70.0640% | 0.00003902\n",
      "Epoch:     3 | 0.6004 | 61.9260% | 0.00003782\n",
      "Epoch:     4 | 0.5906 | 61.2323% | 0.00003618\n",
      "Epoch:     5 | 0.5860 | 61.7868% | 0.00003414\n",
      "Epoch:     6 | 0.5833 | 60.4732% | 0.00003176\n",
      "Epoch:     7 | 0.5794 | 60.5352% | 0.00002908\n",
      "Epoch:     8 | 0.5762 | 60.7629% | 0.00002618\n",
      "Epoch:     9 | 0.5745 | 60.1886% | 0.00002313\n",
      "Epoch:    10 | 0.5707 | 59.9626% | 0.00002001\n",
      "Epoch:    11 | 0.5650 | 59.4160% | 0.00001688\n",
      "Epoch:    12 | 0.5591 | 58.8740% | 0.00001383\n",
      "Epoch:    13 | 0.5533 | 58.5364% | 0.00001093\n",
      "Epoch:    14 | 0.5492 | 58.2907% | 0.00000825\n",
      "Epoch:    15 | 0.5447 | 58.4875% | 0.00000587\n",
      "Epoch:    16 | 0.5414 | 57.9117% | 0.00000383\n",
      "Epoch:    17 | 0.5382 | 57.5221% | 0.00000219\n",
      "Epoch:    18 | 0.5345 | 57.3324% | 0.00000099\n",
      "Epoch:    19 | 0.5323 | 57.1632% | 0.00000026\n",
      "Epoch:    20 | 0.5311 | 57.1373% | 0.00004000\n",
      "Epoch:    21 | 0.5551 | 59.9035% | 0.00003994\n",
      "Epoch:    22 | 0.5497 | 61.6703% | 0.00003975\n",
      "Epoch:    23 | 0.5457 | 58.9369% | 0.00003945\n",
      "Epoch:    24 | 0.5403 | 58.5078% | 0.00003902\n",
      "Epoch:    25 | 0.5342 | 56.8414% | 0.00003848\n",
      "Epoch:    26 | 0.5259 | 56.2232% | 0.00003782\n",
      "Epoch:    27 | 0.5198 | 55.3812% | 0.00003705\n",
      "Epoch:    28 | 0.5105 | 55.2844% | 0.00003618\n",
      "Epoch:    29 | 0.5055 | 53.8816% | 0.00003521\n",
      "Epoch:    30 | 0.4934 | 53.3753% | 0.00003414\n",
      "Epoch:    31 | 0.4838 | 52.3823% | 0.00003299\n",
      "Epoch:    32 | 0.4761 | 52.9411% | 0.00003176\n",
      "Epoch:    33 | 0.4670 | 51.7096% | 0.00003045\n",
      "Epoch:    34 | 0.4563 | 50.6876% | 0.00002908\n",
      "Epoch:    35 | 0.4489 | 50.0294% | 0.00002766\n",
      "Epoch:    36 | 0.4428 | 49.3874% | 0.00002618\n",
      "Epoch:    37 | 0.4349 | 51.8754% | 0.00002467\n",
      "Epoch:    38 | 0.4251 | 48.1652% | 0.00002313\n",
      "Epoch:    39 | 0.4190 | 49.7522% | 0.00002157\n",
      "Epoch:    40 | 0.4122 | 47.2920% | 0.00002001\n",
      "Epoch:    41 | 0.4047 | 47.4391% | 0.00001844\n",
      "Epoch:    42 | 0.3989 | 45.9032% | 0.00001688\n",
      "Epoch:    43 | 0.3909 | 44.8018% | 0.00001534\n",
      "Epoch:    44 | 0.3850 | 44.8139% | 0.00001383\n",
      "Epoch:    45 | 0.3799 | 44.0648% | 0.00001235\n",
      "Epoch:    46 | 0.3739 | 43.6115% | 0.00001093\n",
      "Epoch:    47 | 0.3677 | 43.2539% | 0.00000956\n",
      "Epoch:    48 | 0.3621 | 42.8155% | 0.00000825\n",
      "Epoch:    49 | 0.3572 | 42.3928% | 0.00000702\n",
      "Epoch:    50 | 0.3522 | 42.0830% | 0.00000587\n",
      "Epoch:    51 | 0.3496 | 41.3388% | 0.00000480\n",
      "Epoch:    52 | 0.3456 | 41.2541% | 0.00000383\n",
      "Epoch:    53 | 0.3438 | 40.2584% | 0.00000296\n",
      "Epoch:    54 | 0.3410 | 39.8250% | 0.00000219\n",
      "Epoch:    55 | 0.3397 | 39.2603% | 0.00000153\n",
      "Epoch:    56 | 0.3388 | 38.9269% | 0.00000099\n",
      "Epoch:    57 | 0.3391 | 38.5375% | 0.00000056\n",
      "Epoch:    58 | 0.3391 | 38.2870% | 0.00000026\n",
      "Epoch:    59 | 0.3385 | 38.0810% | 0.00000007\n",
      "Epoch:    60 | 0.3377 | 38.0033% | 0.00004000\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model = train_model_no_val(train_loader, prices_train_raw, epochs=60, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 768) (75000, 768)\n"
     ]
    }
   ],
   "source": [
    "loaded_data = np.load(\"Test_embeddings_image_text.npz\", allow_pickle=True)\n",
    "img_test = loaded_data['img_embds_test']\n",
    "text_test = loaded_data['text_embds_test']\n",
    "print(img_test.shape, text_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict_test_set(model, img_test, text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>15.122637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>15.063874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>22.577444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>6.786475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>22.511200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id      price\n",
       "0     100179  15.122637\n",
       "1     245611  15.063874\n",
       "2     146263  22.577444\n",
       "3      95658   6.786475\n",
       "4      36806  22.511200"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub = test.copy()\n",
    "test_sub = test_sub.drop(['catalog_content', 'image_link'], axis=1)\n",
    "test_sub['price'] = preds\n",
    "test_sub[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub.to_csv(\"sub3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
