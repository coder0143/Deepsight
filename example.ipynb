{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9006034",
   "metadata": {},
   "source": [
    "##  Basic Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "432e4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8811f03",
   "metadata": {},
   "source": [
    "##  Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cad5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = '../dataset/'\n",
    "train = pd.read_csv(os.path.join(DATASET_FOLDER, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(DATASET_FOLDER, 'test.csv'))\n",
    "sample_test = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test.csv'))\n",
    "sample_test_out = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test_out.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "512830da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.84637333333333\n",
      "97.0\n",
      "First Quartile (Q1): 37.0\n",
      "Third Quartile (Q3): 201.0\n",
      "Interquartile Range (IQR): 164.0\n"
     ]
    }
   ],
   "source": [
    "# Max possible sequence length\n",
    "lengths = [len(i.split(' ')) for i in train[\"catalog_content\"]]\n",
    "Q1 = np.percentile(lengths, 25)\n",
    "Q3 = np.percentile(lengths, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(np.mean(lengths))\n",
    "print(np.median(lengths))\n",
    "print(f\"First Quartile (Q1): {Q1}\")\n",
    "print(f\"Third Quartile (Q3): {Q3}\")\n",
    "print(f\"Interquartile Range (IQR): {IQR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae4e0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 38792/75000 [01:11<01:00, 601.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not able to download - https://m.media-amazon.com/images/I/51mjZYDYjyL.jpg\n",
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [02:09<00:00, 576.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import download_images\n",
    "download_images(train['image_link'], '../train_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7231d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 41803/75000 [01:38<01:06, 500.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not able to download - https://m.media-amazon.com/images/I/813CjSgHj0S.jpg\n",
      "HTTP Error 404: Not Found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 41907/75000 [01:38<01:34, 351.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [02:46<00:00, 450.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import download_images\n",
    "download_images(test['image_link'], '../test_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a40251a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  \n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b141f9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_166784/652001188.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "570bf4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa609492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(features, features),\n",
    "            nn.LayerNorm(features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Linear(features, features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a4cd4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPRegressor(nn.Module):\n",
    "    def __init__(self, base_model_name=\"openai/clip-vit-base-patch32\", num_residual_blocks=1):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(base_model_name)\n",
    "            \n",
    "        # Initial projection layer\n",
    "        self.initial_projection = nn.Linear(512 * 2, 64)\n",
    "        \n",
    "        # Stack residual blocks\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, text_inputs, image_inputs):\n",
    "        outputs = self.clip(**text_inputs, **image_inputs)\n",
    "        img_emb = outputs.image_embeds\n",
    "        txt_emb = outputs.text_embeds\n",
    "        fused = torch.cat([img_emb, txt_emb], dim=1)\n",
    "        \n",
    "        # Process through the regression head\n",
    "        x = self.initial_projection(fused)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7b5f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_regressor = CLIPRegressor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e88b9f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze most layers except the last transformer block + projection layers\n",
    "# for name, param in clip_regressor.clip.text_model.named_parameters():\n",
    "#     if not any(x in name for x in [\"encoder.layers.11\", \"final_layer_norm\"]):\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# for name, param in clip_regressor.clip.vision_model.named_parameters():\n",
    "#     if not any(x in name for x in [\"encoder.layers.11\", \"post_layernorm\"]):\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee78407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPRegressionDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_folder, processor_name=\"openai/clip-vit-base-patch32\", max_length=77):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = CLIPProcessor.from_pretrained(processor_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # --- Text ---\n",
    "        text = str(row[\"catalog_content\"])\n",
    "        if len(text.strip()) == 0:\n",
    "            text = \"No description available.\"\n",
    "\n",
    "        # --- Image ---\n",
    "        img_name = row[\"image_link\"].split(\"/\")[-1]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            # Fallback image (solid black) if corrupted or missing\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        # --- Price target ---\n",
    "        price = torch.tensor(float(row[\"price\"]), dtype=torch.float32)\n",
    "\n",
    "        # --- Preprocess with CLIP ---\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Remove batch dim (CLIPProcessor returns 1-batch)\n",
    "        text_inputs = {k: v.squeeze(0) for k, v in inputs.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        image_inputs = {k: v.squeeze(0) for k, v in inputs.items() if k in [\"pixel_values\"]}\n",
    "\n",
    "        return {\n",
    "            \"text_inputs\": text_inputs,\n",
    "            \"image_inputs\": image_inputs,\n",
    "            \"price\": price,\n",
    "        }\n",
    "\n",
    "def collate_fn_train(batch):\n",
    "    \"\"\"Custom collator for batching CLIPProcessor outputs\"\"\"\n",
    "    text = {\n",
    "        \"input_ids\": torch.stack([b[\"text_inputs\"][\"input_ids\"] for b in batch]),\n",
    "        \"attention_mask\": torch.stack([b[\"text_inputs\"][\"attention_mask\"] for b in batch]),\n",
    "    }\n",
    "    images = {\n",
    "        \"pixel_values\": torch.stack([b[\"image_inputs\"][\"pixel_values\"] for b in batch]),\n",
    "    }\n",
    "    prices = torch.stack([b[\"price\"] for b in batch])\n",
    "    return text, images, prices\n",
    "\n",
    "def get_train_dataloader(csv_path, image_folder, batch_size=32, num_workers=4, shuffle=True):\n",
    "    dataset = CLIPRegressionDataset(csv_path, image_folder)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn_train,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50319101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/teamspace/studios/this_studio/student_resource/dataset/train.csv\")\n",
    "price_mean = df[\"price\"].mean()\n",
    "price_std = df[\"price\"].std()\n",
    "df[\"price\"] = (df[\"price\"] - price_mean) / price_std\n",
    "df.to_csv(\"train_norm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1851a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_train_dataloader(\n",
    "    csv_path=\"/teamspace/studios/this_studio/student_resource/src/train_norm.csv\",\n",
    "    image_folder=\"train_images\",\n",
    "    batch_size=32,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3616b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMAPELoss(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-2):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        numerator = torch.abs(preds - targets)\n",
    "        denominator = (torch.abs(targets) + torch.abs(preds)).clamp(min=self.eps)\n",
    "        smape = numerator / (denominator / 2.0)\n",
    "        return smape.mean()\n",
    "\n",
    "smape_loss_fn = SMAPELoss()\n",
    "l1_loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "def hybrid_loss(preds, targets, alpha=0.7):\n",
    "    \"\"\"Blend SMAPE (relative) and L1 (absolute) losses.\"\"\"\n",
    "    return alpha * smape_loss_fn(preds, targets) + (1 - alpha) * l1_loss_fn(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a24512",
   "metadata": {},
   "source": [
    "* Train from further checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e5910e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\"clip_regressor_smape.pt\", weights_only=False) # map_location=device\n",
    "# price_mean, price_std = checkpoint[\"price_mean\"], checkpoint[\"price_std\"]\n",
    "\n",
    "# clip_regressor = CLIPRegressor().to(device)\n",
    "# clip_regressor.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# freezing all the weights, only mlp will be trainable\n",
    "for name, param in clip_regressor.clip.text_model.named_parameters():\n",
    "    if not any(x in name for x in [\"encoder.layers.11\", \"final_layer_norm\"]):\n",
    "        param.requires_grad = False\n",
    "\n",
    "for name, param in clip_regressor.clip.vision_model.named_parameters():\n",
    "    if not any(x in name for x in [\"encoder.layers.11\", \"final_layer_norm\"]):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50ad16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, clip_regressor.parameters()),\n",
    "    lr=5e-5, weight_decay=1e-6  # small weight decay for regularization\n",
    ")\n",
    "\n",
    "scaler = GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04e9dd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfe8121e3894c599403329c74166c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/7:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done | Avg Loss: 0.6590 | Avg SMAPE: 84.31%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6774831005b9409eb0b80d67cd83ae8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/7:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done | Avg Loss: 0.5961 | Avg SMAPE: 76.96%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280885d6590d4b94b31a286d2aebac11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/7:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done | Avg Loss: 0.5779 | Avg SMAPE: 74.76%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c60714522f4cee81f790fd5deb964f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/7:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 done | Avg Loss: 0.5679 | Avg SMAPE: 73.54%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07f180053374439a12b674c4971e838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/7:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 done | Avg Loss: 0.5619 | Avg SMAPE: 72.80%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac863b9809c24f2e8e6275362cefddf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/7:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Mixed precision backward\u001b[39;00m\n\u001b[1;32m     22\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_regressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     25\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:36\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:214\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    212\u001b[0m is_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(parameters, types\u001b[38;5;241m.\u001b[39mGeneratorType)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# prevent generators from being exhausted\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_generator \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parameters) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`parameters` is an empty generator, no gradient clipping will occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    218\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2673\u001b[0m, in \u001b[0;36mModule.parameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Parameter]:\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over module parameters.\u001b[39;00m\n\u001b[1;32m   2653\u001b[0m \n\u001b[1;32m   2654\u001b[0m \u001b[38;5;124;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2671\u001b[0m \n\u001b[1;32m   2672\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2673\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecurse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2706\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \n\u001b[1;32m   2681\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2698\u001b[0m \n\u001b[1;32m   2699\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2700\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_named_members(\n\u001b[1;32m   2701\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   2702\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[1;32m   2703\u001b[0m     recurse\u001b[38;5;241m=\u001b[39mrecurse,\n\u001b[1;32m   2704\u001b[0m     remove_duplicate\u001b[38;5;241m=\u001b[39mremove_duplicate,\n\u001b[1;32m   2705\u001b[0m )\n\u001b[0;32m-> 2706\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2641\u001b[0m, in \u001b[0;36mModule._named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2635\u001b[0m memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   2636\u001b[0m modules \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_modules(prefix\u001b[38;5;241m=\u001b[39mprefix, remove_duplicate\u001b[38;5;241m=\u001b[39mremove_duplicate)\n\u001b[1;32m   2638\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recurse\n\u001b[1;32m   2639\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m [(prefix, \u001b[38;5;28mself\u001b[39m)]\n\u001b[1;32m   2640\u001b[0m )\n\u001b[0;32m-> 2641\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmembers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_members_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmembers\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2863\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2861\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2862\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2863\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2864\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2865\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2863\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2861\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2862\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2863\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2864\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2865\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2863\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2861\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2862\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2863\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2864\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2865\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:2859\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2857\u001b[0m     memo\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2858\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2859\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2861\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 7  # typically enough for partial CLIP fine-tune\n",
    "clip_regressor.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_smape = 0.0\n",
    "\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", dynamic_ncols=True)\n",
    "    \n",
    "    for step, (text_inputs, image_inputs, prices) in enumerate(pbar):\n",
    "        text_inputs = {k: v.to(device, non_blocking=True) for k, v in text_inputs.items()}\n",
    "        image_inputs = {k: v.to(device, non_blocking=True) for k, v in image_inputs.items()}\n",
    "        prices = prices.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            preds = clip_regressor(text_inputs, image_inputs)\n",
    "            loss = hybrid_loss(preds, prices)\n",
    "\n",
    "        # Mixed precision backward\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(clip_regressor.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute running SMAPE in normalized space\n",
    "        with torch.no_grad():\n",
    "            batch_smape = smape_loss_fn(preds, prices).item() * 100\n",
    "        running_smape += batch_smape\n",
    "\n",
    "        if (step + 1) % 50 == 0:\n",
    "            avg_loss = running_loss / (step + 1)\n",
    "            avg_smape = running_smape / (step + 1)\n",
    "            pbar.set_postfix({\"Loss\": f\"{avg_loss:.4f}\", \"SMAPE%\": f\"{avg_smape:.2f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1} done | Avg Loss: {avg_loss:.4f} | Avg SMAPE: {avg_smape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce556327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 💾 Save Model\n",
    "# ------------------------------\n",
    "torch.save({\n",
    "    \"model_state_dict\": clip_regressor.state_dict(),\n",
    "    \"price_mean\": price_mean,\n",
    "    \"price_std\": price_std\n",
    "}, \"clip_regressor_smape.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb6a0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTestDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_folder, processor_name=\"openai/clip-vit-base-patch32\", max_length=77):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = CLIPProcessor.from_pretrained(processor_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row[\"catalog_content\"])\n",
    "        if len(text.strip()) == 0:\n",
    "            text = \"No description available.\"\n",
    "\n",
    "        img_name = row[\"image_link\"].split(\"/\")[-1]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        text_inputs = {k: v.squeeze(0) for k, v in inputs.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        image_inputs = {k: v.squeeze(0) for k, v in inputs.items() if k in [\"pixel_values\"]}\n",
    "\n",
    "        return {\n",
    "            \"sample_id\": row[\"sample_id\"],\n",
    "            \"text_inputs\": text_inputs,\n",
    "            \"image_inputs\": image_inputs\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text = {\n",
    "        \"input_ids\": torch.stack([b[\"text_inputs\"][\"input_ids\"] for b in batch]),\n",
    "        \"attention_mask\": torch.stack([b[\"text_inputs\"][\"attention_mask\"] for b in batch]),\n",
    "    }\n",
    "    images = {\n",
    "        \"pixel_values\": torch.stack([b[\"image_inputs\"][\"pixel_values\"] for b in batch]),\n",
    "    }\n",
    "    sample_ids = [b[\"sample_id\"] for b in batch]\n",
    "    return text, images, sample_ids\n",
    "\n",
    "def get_test_dataloader(csv_path, image_folder, batch_size=32, num_workers=8):\n",
    "    dataset = CLIPTestDataset(csv_path, image_folder)\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a5744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"clipreg_smape.pt\", weights_only=False) # map_location=device\n",
    "price_mean, price_std = checkpoint[\"price_mean\"], checkpoint[\"price_std\"]\n",
    "\n",
    "clip_regressor = CLIPRegressor().to(device)\n",
    "clip_regressor.load_state_dict(checkpoint[\"model_state_dclip_regressor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56bff7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = get_test_dataloader(\"/teamspace/studios/this_studio/student_resource/dataset/test.csv\", \"test_images\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b73ff8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1770f83641134047a4a8b58a5d74843f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions to submission.csv\n"
     ]
    }
   ],
   "source": [
    "clip_regressor.eval()\n",
    "predictions = []\n",
    "sample_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(test_dataloader, desc=\"Predicting\", dynamic_ncols=True)\n",
    "    for text_inputs, image_inputs, sids in pbar:\n",
    "        text_inputs = {k: v.to(device, non_blocking=True) for k, v in text_inputs.items()}\n",
    "        image_inputs = {k: v.to(device, non_blocking=True) for k, v in image_inputs.items()}\n",
    "\n",
    "        with autocast(device_type='cuda',dtype=torch.bfloat16):\n",
    "            preds = clip_regressor(text_inputs, image_inputs)\n",
    "\n",
    "        preds = preds.squeeze(1).cpu()\n",
    "\n",
    "        # Denormalize\n",
    "        preds = preds * price_std + price_mean\n",
    "\n",
    "        # Ensure non-negative prices\n",
    "        preds = torch.clamp(preds, min=0.0)\n",
    "\n",
    "        predictions.extend(preds.tolist())\n",
    "        sample_ids.extend(sids)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": sample_ids,\n",
    "    \"price\": predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ Saved predictions to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51392cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
